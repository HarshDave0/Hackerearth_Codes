{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier,AdaBoostClassifier\n",
    "#from sklearn.naive_bayes import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "\n",
    "# load data\n",
    "train = pd.read_csv(\"E:\\HackerEarth\\Machine_learning_beginers\\\\f2c2f440-8-dataset_he\\\\train.csv\")\n",
    "test = pd.read_csv(\"E:\\HackerEarth\\Machine_learning_beginers\\\\f2c2f440-8-dataset_he\\\\test.csv\")\n",
    "\n",
    "train.head()\n",
    "\n",
    "# function to clean data\n",
    "allowed_word_type = [\"J\"]\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "def cleanData(text, lowercase = True, remove_stops = True, stemming = True):\n",
    "    txt = str(text)\n",
    "    txt = re.sub(r'[^A-Za-z0-9\\s]',r'',txt)\n",
    "    txt = re.sub(r'\\n',r' ',txt)\n",
    "    \n",
    "    if lowercase:\n",
    "        txt = \" \".join([w.lower() for w in txt.split()])\n",
    "        \n",
    "    if remove_stops:\n",
    "        txt = \" \".join([w for w in txt.split() if w not in stops])\n",
    "    \n",
    "    if stemming:\n",
    "        st = PorterStemmer()\n",
    "        txt = \" \".join([st.stem(w) for w in txt.split()])\n",
    "        \n",
    "    return txt\n",
    "\n",
    "## join data\n",
    "test['Is_Response'] = np.nan\n",
    "alldata = pd.concat([train, test]).reset_index(drop=True)\n",
    "\n",
    "# clean description\n",
    "alldata['Description'] = alldata['Description'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=True))\n",
    "\n",
    "# # initialise the functions - we'll create separate models for each type.\n",
    "# countvec = CountVectorizer(analyzer='word', ngram_range = (1,1), min_df=150, max_features=500)\n",
    "# tfidfvec = TfidfVectorizer(analyzer='word', ngram_range = (1,1), min_df = 150, max_features=500)\n",
    "\n",
    "# # create features\n",
    "# bagofwords = countvec.fit_transform(alldata['Description'])\n",
    "# tfidfdata = tfidfvec.fit_transform(alldata['Description'])\n",
    "\n",
    "# # label encode categorical features in data given\n",
    "# cols = ['Browser_Used','Device_Used']\n",
    "\n",
    "# for x in cols:\n",
    "#     lbl = LabelEncoder()\n",
    "#     alldata[x] = lbl.fit_transform(alldata[x])\n",
    "    \n",
    "# # create dataframe for features\n",
    "# bow_df = pd.DataFrame(bagofwords.todense())\n",
    "# tfidf_df = pd.DataFrame(tfidfdata.todense())\n",
    "\n",
    "# # set column names\n",
    "# bow_df.columns = ['col'+ str(x) for x in bow_df.columns]\n",
    "# tfidf_df.columns = ['col' + str(x) for x in tfidf_df.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        room kind clean strong smell dog gener averag ...\n",
       "1        stay crown plaza april april staff friendli at...\n",
       "2        book hotel hotwir lowest price could find got ...\n",
       "3        stay husband son way alaska cruis love hotel g...\n",
       "4        girlfriend stay celebr th birthday plan weeken...\n",
       "5        room one nice clearli updat recent clean bed c...\n",
       "6        husband stay hotel time though fanciest hotel ...\n",
       "7        wife stay gloriou citi back sf expens found li...\n",
       "8        boyfriend stay fairmont recent trip san franci...\n",
       "9        wonder staff great locat defin price high stan...\n",
       "10       step time squar nice room stay night great sho...\n",
       "11       wife kid stay valentin weekend realli nice hot...\n",
       "12       stay jolli madison xma period main featur loca...\n",
       "13       highli recommend hawthorn terrac afford comfor...\n",
       "14       found hotel clean nice locat good free shuttl ...\n",
       "15       stay elan th th octob like much return day tri...\n",
       "16       pricelin sent us hotel accept bid usd night ta...\n",
       "17       old cheap furnituresour chair simpli destroyed...\n",
       "18       stay night realli happi locat min walk walk fa...\n",
       "19       servic fine hotel fell way expect respect esta...\n",
       "20       stay mani hilton properti expect friendli effi...\n",
       "21       everyth could want hotel conveni locat short w...\n",
       "22       much want stay boutiqu hotel near time squar l...\n",
       "23       realli like hotel staff wonder quit help provi...\n",
       "24       wife spent day month new york getaway vacat ch...\n",
       "25       stay hotel two night busi labor day weekend ar...\n",
       "26       took girl trip la idea stay research settl ela...\n",
       "27       stay girlfriend long weekend gave us nice comp...\n",
       "28       stay numer time never disappoint staff help ov...\n",
       "29       public area nice look staff recept help checki...\n",
       "                               ...                        \n",
       "68306    palomar nice hotel noth special decor bit gene...\n",
       "68307    part good hotel minus common hotel instanc mak...\n",
       "68308    book hotel pricelin given tini room doubl bed ...\n",
       "68309    pay top price room pay wifi servic room ridicu...\n",
       "68310    stay march san antonio confer convent center r...\n",
       "68311    realli conveni locat walk distanc time squaret...\n",
       "68312    room clean staff went beyond friendli help rec...\n",
       "68313    phone live area work fix staysink bathroom dra...\n",
       "68314    travel throughout world work know four star ho...\n",
       "68315    got toogoodtobetru rate motor club kept wait s...\n",
       "68316    got back morn new york want review hotel strai...\n",
       "68317    book doubl hotal room arriv late find small ai...\n",
       "68318    high mayb that go rate motel day got upstair r...\n",
       "68319    properti alway come degre excel alway expect r...\n",
       "68320    husband frequent hotel make good time everi vi...\n",
       "68321    hotel great locat day stay stay overnight flig...\n",
       "68322    husband spend labor day weekend nyc found hote...\n",
       "68323    problem hotel room outsid lobbi hotel look qui...\n",
       "68324    hotel icon welldesign hotel built former histo...\n",
       "68325    book hotel tomo got good san francisco price e...\n",
       "68326    hotel may glamor sf realli comfort free park f...\n",
       "68327    stay night room overlook bu depot wasnt greate...\n",
       "68328    stay three night june locat good walk penn sta...\n",
       "68329    spent night king castia bed wonder clean staff...\n",
       "68330    book flighthotel packag expedia end great rate...\n",
       "68331    stay hotel tower confer love place room spacio...\n",
       "68332    tri stay within marriott famili want pay park ...\n",
       "68333    stay night littl dogveri friendli staff ask se...\n",
       "68334    stay yotel weekend impress thing like best sof...\n",
       "68335    blake comfort everi way room nice furnish lnen...\n",
       "Name: Description, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "room kind clean strong smell dog gener averag ok overnight stay your fussi would consid stay price right breakfast free better noth /n /n\n",
      "stay crown plaza april april staff friendli attent elev tini food restaur delici price littl high side cours washington dc pool littl children room fifth floor two comfort bed plenti space one person tv littl small today standard limit number channel small bit mold bathtub area could remov littl bleach appear carpet vacum everi day report light bulb burn never replac ice machin odd number floor one floor work encount staff elev one even mention ice machin severel hour later maid appear door ice two mint im sure knew room littl unnerv would stay busi would come vacat /n /n\n",
      "book hotel hotwir lowest price could find got front desk manag gave us smoke room argu littl babi would book room known smoke manag would hear anyth told hotwir book cheapest room avail get go unhappi great deal persuas discuss got nonsmok room thereaft room minim amen besid great locat near dupont circl much say overpr hotel room small ok condit bathroom small tub bathroom amen also minim fridg microwav rent fridg staff keep babi thing park cost per day best drive also breakfast includ lobbi small feel old thing avail coffe lobbi decent poor servic minim amen small room small bathroom view great locat distanc metro either mcpherson station dupont station tri look better avail /n /n\n",
      "stay husband son way alaska cruis love hotel great experi ask room north tower face north west best view high floor stun view needl citi even cruis ship order room servic dinner could enjoy perfect view room servic dinner delici perfect spot walk everywher enjoy citi almost forgot heavenli bed heavenli /n /n\n",
      "girlfriend stay celebr th birthday plan weekend getaway back februari look forward us rent ajoin room rate nightroom stay night tax park charg spent total us expect certain level servic sinc stay ritz howev hotel certainli didnt meet would consid ritz standard room fine clean well appoint bathroom noth get excit servic howev aw first night made call bell man someon hotel commun pick phone saturday night left dinner request extra hotel deliv room trash remov return neither request fullfil made anoth call housekeep final thing taken care opinion rate pay nightli turn servic speak manag check said would make worth return howev receiv letter apolog needless say plan return hotel husband stay four season boston februari wonder stay servic impecc servic four seaon stay will pay ritz carlton price would recommend stay four season instead /n /n\n",
      "room one nice clearli updat recent clean bed comfi need updat carpet old wrinkl exampl great locat visit inner harbor get fell point oriol game etc supershuttl bwi work great way tv remot room terribl didnt watch much tv big deal wireless sketchi th th floor didnt need much vacat didnt realli matter breakfast good morn would stay town /n /n\n",
      "husband stay hotel time though fanciest hotel love fact walk mile fenway clean staff accomod complaint fan bathroom noisi went automat turn light tri keep light much possibl weve stay pricier hotel charg internet breakfast includ stay /n /n\n",
      "wife stay gloriou citi back sf expens found littl hotel super locat reason price mean star hotel room clean nice wellappoint quaint charm despit bit small regardless staff help particularli enjoy winetast even help polit front desk overal id recommend place singl coupl children present probabl need bit bigger room locat proxim everyth includ marvel littl mexican restaur around corner cant beat place price sf magic citi plenti thing id recommend crash see wonder surround hang room day would recommend getaway famili room size /n /n\n",
      "boyfriend stay fairmont recent trip san francisco could recommend hotel call hotel week order cake deliv room first night celebr boyfriend birthday immedi connect directli pastri chef help design delici cake ever eaten entir life room larg luxuri wonder oldworld feel importantli dont miss dinner tonga room fun restaur definit order scorpion bowl your stay hotel theyr strong /n /n\n",
      "wonder staff great locat defin price high standard hotel free breakfast great actual pretti good quailti free buffet hotel heart walk distanc everyth thing pleas hyattth coffe bad white ginger line shampooconditionersoap smell horribl /n /n\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(alldata['Description'][i],'/n','/n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "room kind clean strong smell dog gener ok overnight fussi price right free noth /n \n",
      "\n",
      "crown april staff friendli attent elev tini food restaur delici price littl high side dc pool littl room fifth floor comfort plenti space person tv littl small today standard limit number small bit mold bathtub area littl bleach carpet vacum everi day report bulb burn never ice machin number floor floor work encount staff even mention ice hour later appear door ice mint im sure room littl unnerv busi vacat /n \n",
      "\n",
      "book hotel hotwir price got front desk manag smoke room argu littl babi book room smoke manag anyth hotwir book cheapest room avail get unhappi great deal persuas discuss nonsmok room thereaft room minim great locat dupont circl much overpr hotel room small ok condit bathroom small tub bathroom also fridg microwav rent fridg staff babi thing park cost day drive also includ lobbi small old thing avail coffe decent poor servic minim small room small bathroom view great locat distanc metro mcpherson station dupont station tri avail /n \n",
      "\n",
      "stay son way hotel great experi ask room north tower face north west view high floor stun view citi even ship order room servic dinner perfect view room servic dinner delici perfect spot everywher citi almost heavenli bed heavenli /n \n",
      "\n",
      "girlfriend stay celebr th birthday plan weekend getaway back februari look forward ajoin room rate nightroom stay night tax park charg total certain level servic sinc stay ritz howev hotel certainli didnt meet ritz standard room fine clean well appoint bathroom excit servic howev aw first night call bell man someon hotel commun pick phone saturday night dinner request extra hotel deliv room trash remov return request fullfil call housekeep final thing care opinion rate pay turn servic speak manag check worth return howev letter apolog needless plan hotel husband season boston februari stay servic impecc seaon stay carlton price season instead /n \n",
      "\n",
      "room nice clearli updat recent clean bed updat carpet old wrinkl great locat visit inner harbor get point oriol game supershuttl bwi work great way tv remot room terribl didnt watch much tv big deal wireless sketchi th th floor need much vacat didnt matter breakfast good morn town /n \n",
      "\n",
      "husband hotel time fanciest hotel love fact fenway clean staff accomod complaint fan bathroom automat turn light tri light much possibl stay pricier hotel charg internet breakfast includ stay /n \n",
      "\n",
      "wife stay gloriou citi back sf littl hotel super locat reason price mean star hotel room clean nice wellappoint quaint charm despit bit small regardless staff help particularli even front desk overal id place singl coupl present bit room locat proxim everyth includ marvel littl mexican restaur corner cant beat place price sf magic citi plenti thing id recommend crash see surround hang room day getaway famili room size /n \n",
      "\n",
      "boyfriend stay fairmont recent trip san francisco hotel call hotel week order deliv room first night celebr boyfriend birthday connect directli pastri chef help design delici ever entir life room larg luxuri wonder oldworld feel importantli dont miss dinner tonga room fun restaur definit order scorpion stay hotel theyr strong /n \n",
      "\n",
      "wonder staff great locat defin price high standard hotel free breakfast great actual pretti good quailti free buffet hotel heart distanc everyth thing coffe bad white ginger line shampooconditionersoap smell horribl /n \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier,AdaBoostClassifier\n",
    "#from sklearn.naive_bayes import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "allowed_word_type = [\"JJ\",\"RB\",\"NN\"]\n",
    "from nltk import pos_tag\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "def cleanData(text, lowercase = True, remove_stops = True, stemming = True):\n",
    "    txt = str(text)\n",
    "    txt = re.sub(r'[^A-Za-z0-9\\s]',r'',txt)\n",
    "    txt = re.sub(r'\\n',r' ',txt)\n",
    "    \n",
    "    if lowercase:\n",
    "        txt = \" \".join([w.lower() for w in txt.split()])\n",
    "        \n",
    "    if remove_stops:\n",
    "        txt = \" \".join([w for w in txt.split() if w not in stops])\n",
    "    \n",
    "    if stemming:\n",
    "        st = PorterStemmer()\n",
    "        txt = \" \".join([st.stem(w) for w in txt.split()])\n",
    "        \n",
    "    words = word_tokenize(txt)\n",
    "    pos = pos_tag(words)\n",
    "    c = []\n",
    "    for w in pos:\n",
    "        if w[1] in allowed_word_type:\n",
    "            c.append(w[0])\n",
    "    x = ' '.join(word for word in c)\n",
    "    return x\n",
    "\n",
    "test['Is_Response'] = np.nan\n",
    "alldata = pd.concat([train, test]).reset_index(drop=True)\n",
    "alldata['Description'] = alldata['Description'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=True))\n",
    "for i in range(10):\n",
    "    print(alldata['Description'][i],'/n','\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1532df42d930>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'c' is not defined"
     ]
    }
   ],
   "source": [
    "txt = \" \"\n",
    "for w in c:\n",
    "    txt.join(w)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-d3b99dde80c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[1;31m## join data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Is_Response'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0malldata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "def cleanData(text, lowercase = True, remove_stops = True, stemming = True):\n",
    "    txt = str(text)\n",
    "    txt = re.sub(r'[^A-Za-z0-9\\s]',r'',txt)\n",
    "    txt = re.sub(r'\\n',r' ',txt)\n",
    "    \n",
    "    if lowercase:\n",
    "        txt = \" \".join([w.lower() for w in txt.split()])\n",
    "        \n",
    "    if remove_stops:\n",
    "        txt = \" \".join([w for w in txt.split() if w not in stops])\n",
    "    \n",
    "    if stemming:\n",
    "        st = PorterStemmer()\n",
    "        txt = \" \".join([st.stem(w) for w in txt.split()])\n",
    "    for w in txt.split():\n",
    "        if w[1][0] in allowed_word_type:\n",
    "            txt = \" \".join(w)\n",
    "    print(txt)\n",
    "    return txt\n",
    "\n",
    "## join data\n",
    "test['Is_Response'] = np.nan\n",
    "alldata = pd.concat([train, test]).reset_index(drop=True)\n",
    "\n",
    "# clean description\n",
    "alldata['Description'] = alldata['Description'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 169)\t1\n",
      "  (0, 369)\t1\n",
      "  (0, 337)\t1\n",
      "  (0, 307)\t1\n",
      "  (0, 180)\t1\n",
      "  (0, 401)\t1\n",
      "  (0, 71)\t1\n",
      "  (0, 235)\t1\n",
      "  (0, 372)\t1\n",
      "  (1, 50)\t1\n",
      "  (1, 428)\t1\n",
      "  (1, 218)\t1\n",
      "  (1, 124)\t1\n",
      "  (1, 243)\t1\n",
      "  (1, 209)\t1\n",
      "  (1, 275)\t1\n",
      "  (1, 142)\t1\n",
      "  (1, 491)\t1\n",
      "  (1, 262)\t1\n",
      "  (1, 216)\t3\n",
      "  (1, 294)\t1\n",
      "  (1, 104)\t1\n",
      "  (1, 144)\t1\n",
      "  (1, 58)\t1\n",
      "  (1, 18)\t1\n",
      "  :\t:\n",
      "  (68335, 423)\t1\n",
      "  (68335, 354)\t1\n",
      "  (68335, 411)\t1\n",
      "  (68335, 184)\t1\n",
      "  (68335, 297)\t2\n",
      "  (68335, 57)\t1\n",
      "  (68335, 362)\t3\n",
      "  (68335, 434)\t1\n",
      "  (68335, 117)\t1\n",
      "  (68335, 477)\t1\n",
      "  (68335, 383)\t2\n",
      "  (68335, 10)\t1\n",
      "  (68335, 34)\t2\n",
      "  (68335, 253)\t2\n",
      "  (68335, 186)\t1\n",
      "  (68335, 112)\t1\n",
      "  (68335, 174)\t1\n",
      "  (68335, 208)\t1\n",
      "  (68335, 43)\t2\n",
      "  (68335, 144)\t1\n",
      "  (68335, 18)\t1\n",
      "  (68335, 400)\t1\n",
      "  (68335, 80)\t1\n",
      "  (68335, 391)\t1\n",
      "  (68335, 372)\t3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "# initialise the functions - we'll create separate models for each type.\n",
    "countvec = CountVectorizer(analyzer='word', ngram_range = (1,1), min_df=150, max_features=500)\n",
    "tfidfvec = TfidfVectorizer(analyzer='word', ngram_range = (1,1), min_df = 150, max_features=500)\n",
    "\n",
    "# create features\n",
    "bagofwords = countvec.fit_transform(alldata['Description'])\n",
    "tfidfdata = tfidfvec.fit_transform(alldata['Description'])\n",
    "\n",
    "# label encode categorical features in data given\n",
    "cols = ['Browser_Used','Device_Used']\n",
    "\n",
    "for x in cols:\n",
    "    lbl = LabelEncoder()\n",
    "    alldata[x] = lbl.fit_transform(alldata[x])\n",
    "    \n",
    "# create dataframe for features\n",
    "bow_df = pd.DataFrame(bagofwords.todense())\n",
    "tfidf_df = pd.DataFrame(tfidfdata.todense())\n",
    "\n",
    "# set column names\n",
    "bow_df.columns = ['col'+ str(x) for x in bow_df.columns]\n",
    "tfidf_df.columns = ['col' + str(x) for x in tfidf_df.columns]\n",
    "print(bagofwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier,AdaBoostClassifier\n",
    "#from sklearn.naive_bayes import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "\n",
    "# load data\n",
    "train = pd.read_csv(\"E:\\HackerEarth\\Machine_learning_beginers\\\\f2c2f440-8-dataset_he\\\\train.csv\")\n",
    "test = pd.read_csv(\"E:\\HackerEarth\\Machine_learning_beginers\\\\f2c2f440-8-dataset_he\\\\test.csv\")\n",
    "\n",
    "train.head()\n",
    "\n",
    "# function to clean data\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "def cleanData(text, lowercase = False, remove_stops = False, stemming = False):\n",
    "    txt = str(text)\n",
    "    txt = re.sub(r'[^A-Za-z0-9\\s]',r'',txt)\n",
    "    txt = re.sub(r'\\n',r' ',txt)\n",
    "    \n",
    "    if lowercase:\n",
    "        txt = \" \".join([w.lower() for w in txt.split()])\n",
    "        \n",
    "    if remove_stops:\n",
    "        txt = \" \".join([w for w in txt.split() if w not in stops])\n",
    "    \n",
    "    if stemming:\n",
    "        st = PorterStemmer()\n",
    "        txt = \" \".join([st.stem(w) for w in txt.split()])\n",
    "\n",
    "    return txt\n",
    "\n",
    "## join data\n",
    "test['Is_Response'] = np.nan\n",
    "alldata = pd.concat([train, test]).reset_index(drop=True)\n",
    "\n",
    "# clean description\n",
    "alldata['Description'] = alldata['Description'].map(lambda x: cleanData(x, lowercase=True, remove_stops=True, stemming=True))\n",
    "\n",
    "# initialise the functions - we'll create separate models for each type.\n",
    "countvec = CountVectorizer(analyzer='word', ngram_range = (1,1), min_df=150, max_features=500)\n",
    "tfidfvec = TfidfVectorizer(analyzer='word', ngram_range = (1,1), min_df = 150, max_features=500)\n",
    "\n",
    "# create features\n",
    "bagofwords = countvec.fit_transform(alldata['Description'])\n",
    "tfidfdata = tfidfvec.fit_transform(alldata['Description'])\n",
    "\n",
    "# label encode categorical features in data given\n",
    "cols = ['Browser_Used','Device_Used']\n",
    "\n",
    "for x in cols:\n",
    "    lbl = LabelEncoder()\n",
    "    alldata[x] = lbl.fit_transform(alldata[x])\n",
    "    \n",
    "# create dataframe for features\n",
    "bow_df = pd.DataFrame(bagofwords.todense())\n",
    "tfidf_df = pd.DataFrame(tfidfdata.todense())\n",
    "\n",
    "# set column names\n",
    "bow_df.columns = ['col'+ str(x) for x in bow_df.columns]\n",
    "tfidf_df.columns = ['col' + str(x) for x in tfidf_df.columns]\n",
    "\n",
    "# create separate data frame for bag of words and tf-idf\n",
    "\n",
    "bow_df_train = bow_df[:len(train)]\n",
    "bow_df_test = bow_df[len(train):]\n",
    "\n",
    "tfid_df_train = tfidf_df[:len(train)]\n",
    "tfid_df_test = tfidf_df[len(train):]\n",
    "\n",
    "# split the merged data file into train and test respectively\n",
    "train_feats = alldata[~pd.isnull(alldata.Is_Response)]\n",
    "test_feats = alldata[pd.isnull(alldata.Is_Response)]\n",
    "\n",
    "### set target variable\n",
    "\n",
    "train_feats['Is_Response'] = [1 if x == 'happy' else 0 for x in train_feats['Is_Response']]\n",
    "\n",
    "# merge count (bag of word) features into train\n",
    "train_feats1 = pd.concat([train_feats[cols], bow_df_train], axis = 1)\n",
    "test_feats1 = pd.concat([test_feats[cols], bow_df_test], axis=1)\n",
    "\n",
    "test_feats1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# merge into a new data frame with tf-idf features\n",
    "train_feats2 = pd.concat([train_feats[cols], tfid_df_train], axis=1)\n",
    "test_feats2 = pd.concat([test_feats[cols], tfid_df_test], axis=1)\n",
    "\n",
    "train_feats_both = pd.concat([train_feats[cols], bow_df_train,tfid_df_train], axis = 1)\n",
    "test_feats_both = pd.concat([test_feats[cols], bow_df_test,tfid_df_test], axis=1)\n",
    "\n",
    "target = train_feats['Is_Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train and test sets for blending.\n",
      "0 RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "1 RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "2 ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "3 ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "4 GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.05, loss='deviance', max_depth=6,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "              presort='auto', random_state=None, subsample=0.5, verbose=0,\n",
      "              warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "Blending.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "# import load_data\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "\n",
    "# def logloss(attempt, actual, epsilon=1.0e-15):\n",
    "#     \"\"\"Logloss, i.e. the score of the bioresponse competition.\n",
    "#     \"\"\"\n",
    "#     attempt = np.clip(attempt, epsilon, 1.0-epsilon)\n",
    "#     return - np.mean(actual * np.log(attempt) +\n",
    "#                      (1.0 - actual) * np.log(1.0 - attempt))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    np.random.seed(0)  # seed to shuffle the train set\n",
    "\n",
    "    n_folds = 10\n",
    "    verbose = True\n",
    "    shuffle = False\n",
    "\n",
    "    X = train_feats1\n",
    "    y = target\n",
    "    X_submission = test_feats1\n",
    "\n",
    "    if shuffle:\n",
    "        idx = np.random.permutation(y.size)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    skf = list(StratifiedKFold(y, n_folds))\n",
    "\n",
    "    clfs = [RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "            RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "            ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "            GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=50)]\n",
    "\n",
    "    print (\"Creating train and test sets for blending.\")\n",
    "\n",
    "    dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
    "    dataset_blend_test = np.zeros((X_submission.shape[0], len(clfs)))\n",
    "\n",
    "    for j, clf in enumerate(clfs):\n",
    "        print (j, clf)\n",
    "        dataset_blend_test_j = np.zeros((X_submission.shape[0], len(skf)))\n",
    "        for i, (train, test) in enumerate(skf):\n",
    "            print (\"Fold\", i)\n",
    "            X_train = X.iloc[train]\n",
    "            y_train = y.iloc[train]\n",
    "            X_test = X.iloc[test]\n",
    "            y_test = y.iloc[test]\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_submission = clf.predict_proba(X_test)[:, 1]\n",
    "            dataset_blend_train[test, j] = y_submission\n",
    "            dataset_blend_test_j[:, i] = clf.predict_proba(X_submission)[:, 1]\n",
    "        dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n",
    "    pickle.dump(dataset_blend_train,open(\"train_stack.pickle\",\"wb\"))\n",
    "    pickle.dump(dataset_blend_test,open(\"test_stack.pickle\",\"wb\"))\n",
    "    \n",
    "    print (\"Blending.\")\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(dataset_blend_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86224699476009448"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(dataset_blend_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.85708783  0.86116106  0.86193167  0.86745441  0.86321603]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(clf, dataset_blend_train, target, cv=5, scoring=make_scorer(accuracy_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8641734305969383"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "clf = XGBClassifier()\n",
    "clf.fit(dataset_blend_train, y)\n",
    "clf.score(dataset_blend_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
